"""
AIæ–°é—»æ”¶é›†å™¨
ä½¿ç”¨AI APIæ¥è·å–æœ€æ–°çš„AIé¢†åŸŸæ–°é—»
"""

import requests
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import re
import logging
import time
import urllib3
import os
import httpx

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from .base import BaseCollector, CollectorItem
from config import settings  # ğŸ”„ ä¿®å¤å¯¼å…¥ï¼šä½¿ç”¨settingsè€Œä¸æ˜¯Config


class AINewsCollector(BaseCollector):
    """AIæ–°é—»æ”¶é›†å™¨"""

    def __init__(self, name: str = "ai_news"):
        """
        åˆå§‹åŒ–AIæ–°é—»æ”¶é›†å™¨

        Args:
            name (str): æ”¶é›†å™¨åç§°
        """
        super().__init__()
        self.name = name

        # ä»é…ç½®è·å–APIè®¾ç½®
        self.api_url = settings.deepseek_api_url  # ğŸ”„ ä½¿ç”¨settings
        self.api_key = settings.deepseek_api_key  # ğŸ”„ ä½¿ç”¨settings
        self.model = settings.ai_model  # ğŸ”„ ä½¿ç”¨å¯é…ç½®çš„æ¨¡å‹

        # åˆ›å»ºlogger
        self.logger = logging.getLogger(__name__)

        # ä½¿ç”¨httpxæ›¿ä»£requestsï¼Œæ”¯æŒHTTP/2
        import ssl
        
        # åˆ›å»ºè‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        ssl_context.set_ciphers('DEFAULT@SECLEVEL=1')
        
        # åˆ›å»ºHTTPå®¢æˆ·ç«¯ï¼Œä¸å¼ºåˆ¶ä½¿ç”¨HTTP/2
        self.client = httpx.AsyncClient(
            http2=False,  # ç¦ç”¨HTTP/2ï¼Œä½¿ç”¨HTTP/1.1ç¡®ä¿ç¨³å®šæ€§
            verify=False,  # ç¦ç”¨SSLéªŒè¯
            timeout=httpx.Timeout(
                connect=30.0,  # è¿æ¥è¶…æ—¶
                read=600.0,    # è¯»å–è¶…æ—¶å¢åŠ åˆ°10åˆ†é’Ÿ
                write=30.0,    # å†™å…¥è¶…æ—¶
                pool=60.0      # è¿æ¥æ± è¶…æ—¶
            ),
            limits=httpx.Limits(
                max_keepalive_connections=1,  # ä¿æŒè¿æ¥æ•°
                max_connections=1,            # æœ€å¤§è¿æ¥æ•°
                keepalive_expiry=60.0        # è¿æ¥ä¿æŒæ—¶é—´
            )
        )
        
        # é…ç½®ä»£ç†è®¾ç½® (å®¹å™¨å†…é€šå¸¸ä¸éœ€è¦ä»£ç†)
        proxy_url = os.getenv('https_proxy') or os.getenv('HTTPS_PROXY')
        if proxy_url:
            # httpxä»£ç†é…ç½®
            self.client = httpx.AsyncClient(
                http2=False,
                verify=False,
                proxies=proxy_url,
                timeout=httpx.Timeout(connect=30.0, read=600.0, write=30.0, pool=60.0),
                limits=httpx.Limits(max_keepalive_connections=1, max_connections=1, keepalive_expiry=60.0)
            )
            self.logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
        else:
            self.logger.info("æœªé…ç½®ä»£ç†ï¼Œä½¿ç”¨HTTP/1.1ç›´è¿")

        self.logger.info(f"AIæ–°é—»æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ: {self.name}")
        self.logger.info(f"API URL: {self.api_url}")
        self.logger.info(f"API Key: {self.api_key[:20]}...")
        self.logger.info(f"ä½¿ç”¨æ¨¡å‹: {self.model}")  # ğŸ”„ è®°å½•ä½¿ç”¨çš„æ¨¡å‹

    def get_source_name(self) -> str:
        return "AIæ–°é—»åŠ©æ‰‹"

    async def collect(self) -> List[CollectorItem]:
        """æ”¶é›†AIæ–°é—»é¡¹ç›®"""
        try:
            # å‡†å¤‡APIè¯·æ±‚
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}",
            }

            # æ„å»ºæé—®è¯
            today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
            prompt = f"è¯·å…ˆæœç´¢ç½‘ç»œè·å–{today}çš„æœ€æ–°AIæ–°é—»ï¼Œç„¶åæ•´ç†æœ€è¿‘10æ¡AIæ–°é—»ã€‚å¼€å¤´æ ‡æ³¨å½“å¤©æ—¥æœŸã€‚å†…å®¹ä¼˜å…ˆå…³æ³¨OpenAIã€Claudeã€Googleã€Grokç­‰å¤§å‚çš„æœ€æ–°åŠ¨æ€ã€‚è¯·ç¡®ä¿æœç´¢åˆ°çœŸå®çš„æœ€æ–°ä¿¡æ¯ï¼Œæ–°é—»å†…å®¹ç®€æ´æ¸…æ™°ï¼Œæ— éœ€å¤šä½™è§£é‡Šã€‚"

            data = {
                "max_tokens": 1200,
                "model": self.model,
                "temperature": 0.8,
                "top_p": 1,
                "presence_penalty": 1,
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»æ”¶é›†åŠ©æ‰‹ï¼Œå…·æœ‰ç½‘ç»œæœç´¢èƒ½åŠ›ã€‚è¯·å…ˆæœç´¢ç½‘ç»œè·å–æœ€æ–°çš„AIè¡Œä¸šæ–°é—»ï¼Œç„¶åæ ¹æ®ç”¨æˆ·è¦æ±‚æ•´ç†ï¼Œå†…å®¹è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰ä»·å€¼ã€‚ç¡®ä¿ä¿¡æ¯æ¥æºäºçœŸå®çš„æœ€æ–°ç½‘ç»œæœç´¢ç»“æœã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
            }

            # å‘é€APIè¯·æ±‚ï¼Œä½¿ç”¨ä¸ test_official_api.py ç›¸åŒçš„æˆåŠŸé…ç½®
            import requests
            import os
            
            self.logger.info("å‘é€APIè¯·æ±‚ï¼ˆç®€åŒ–ç›´è¿æ¨¡å¼ï¼‰...")
            response = requests.post(
                self.api_url, 
                headers=headers, 
                data=json.dumps(data).encode('utf-8'),
                proxies={'http': None, 'https': None}  # ç¦ç”¨ä»£ç†
            )
            
            if response.status_code == 200:
                # ç¬¬ä¸€æ­¥ï¼šæ¥æ”¶åŸå§‹æ•°æ®
                self.logger.info("=" * 50)
                self.logger.info("æ­¥éª¤1: æ¥æ”¶APIå“åº”æ•°æ®")
                
                try:
                    result = response.content.decode("utf-8")
                    self.logger.info(f"âœ… æˆåŠŸæ¥æ”¶åŸå§‹å“åº”ï¼Œæ•°æ®é•¿åº¦: {len(result)} å­—ç¬¦")
                    self.logger.info(f"ğŸ“„ åŸå§‹å“åº”å‰1000å­—ç¬¦:")
                    self.logger.info(result[:1000])
                    self.logger.info("=" * 50)
                    
                except Exception as e:
                    self.logger.error(f"âŒ æ­¥éª¤1å¤±è´¥ - æ•°æ®æ¥æ”¶é”™è¯¯: {e}")
                    return self._create_fallback_news()
                
                # ç¬¬äºŒæ­¥ï¼šè§£æJSONç»“æ„
                self.logger.info("æ­¥éª¤2: è§£æJSONå“åº”ç»“æ„")
                try:
                    response_data = json.loads(result)
                    self.logger.info(f"âœ… JSONè§£ææˆåŠŸ")
                    self.logger.info(f"ğŸ“Š å“åº”ç»“æ„é”®: {list(response_data.keys())}")
                    
                    if 'choices' in response_data:
                        choices = response_data.get('choices', [])
                        self.logger.info(f"âœ… æ‰¾åˆ°choicesæ•°ç»„ï¼Œé•¿åº¦: {len(choices)}")
                        if choices and 'message' in choices[0]:
                            self.logger.info(f"âœ… æ‰¾åˆ°messageå¯¹è±¡")
                        else:
                            self.logger.warning(f"âš ï¸ choicesç»“æ„å¼‚å¸¸: {choices}")
                    else:
                        self.logger.error(f"âŒ å“åº”ä¸­ç¼ºå°‘choiceså­—æ®µ")
                        
                except Exception as e:
                    self.logger.error(f"âŒ æ­¥éª¤2å¤±è´¥ - JSONè§£æé”™è¯¯: {e}")
                    self.logger.error(f"åŸå§‹æ•°æ®: {result[:500]}")
                    return self._create_fallback_news()
                
                # ç¬¬ä¸‰æ­¥ï¼šæå–AIå†…å®¹
                self.logger.info("æ­¥éª¤3: æå–AIç”Ÿæˆå†…å®¹")
                try:
                    content = response_data.get('choices', [{}])[0].get('message', {}).get('content', '')
                    self.logger.info(f"âœ… æˆåŠŸæå–AIå†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                    
                    if content:
                        self.logger.info(f"ğŸ“ AIå†…å®¹å‰500å­—ç¬¦:")
                        self.logger.info(content[:500])
                        self.logger.info("=" * 50)
                    else:
                        self.logger.error(f"âŒ æ­¥éª¤3å¤±è´¥ - AIå†…å®¹ä¸ºç©º")
                        return self._create_fallback_news()
                        
                except Exception as e:
                    self.logger.error(f"âŒ æ­¥éª¤3å¤±è´¥ - å†…å®¹æå–é”™è¯¯: {e}")
                    return self._create_fallback_news()
                
                # ç¬¬å››æ­¥ï¼šè¿›å…¥å†…å®¹è§£æå™¨
                self.logger.info("æ­¥éª¤4: è¿›å…¥AIå†…å®¹è§£æå™¨")
                try:
                    news_items = self._parse_ai_response(content)
                    if news_items:
                        self.logger.info(f"âœ… è§£ææˆåŠŸï¼Œç”Ÿæˆ {len(news_items)} æ¡æ–°é—»")
                        return news_items
                    else:
                        self.logger.error(f"âŒ æ­¥éª¤4å¤±è´¥ - è§£æå™¨è¿”å›ç©ºç»“æœ")
                        return self._create_fallback_news()
                        
                except Exception as e:
                    self.logger.error(f"âŒ æ­¥éª¤4å¤±è´¥ - è§£æå™¨å¤„ç†é”™è¯¯: {e}")
                    return self._create_fallback_news()
            else:
                self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}, {response.text}")
                return []

        except (requests.ConnectionError, requests.ReadTimeout, requests.Timeout) as e:
            self.logger.error(f"HTTPè¿æ¥é”™è¯¯: {str(e)}")
            return self._create_fallback_news()

        except Exception as e:
            self.logger.error(f"æ”¶é›†AIæ–°é—»æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return self._create_fallback_news()

    def _create_fallback_news(self) -> List[CollectorItem]:
        """åˆ›å»ºå¤‡ç”¨æ–°é—»æ•°æ®ï¼ˆå½“APIè°ƒç”¨å¤±è´¥æ—¶ï¼‰"""
        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

        fallback_items = [
            CollectorItem(
                title=f"AIæ–°é—»æ”¶é›†æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ - {today}",
                summary="ç”±äºç½‘ç»œè¿æ¥é—®é¢˜ï¼ŒAIæ–°é—»æ”¶é›†æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åé‡è¯•ã€‚",
                content="AIæ–°é—»æ”¶é›†æœåŠ¡é‡åˆ°ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå¯èƒ½æ˜¯SSLè¯ä¹¦éªŒè¯æˆ–ç½‘ç»œè¶…æ—¶å¯¼è‡´ã€‚ç³»ç»Ÿå°†åœ¨ä¸‹æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨é‡è¯•ã€‚",
                url="",
                source=self.get_source_name(),
                author="ç³»ç»Ÿ",
                published_at=datetime.now(),
                tags=["ç³»ç»Ÿé€šçŸ¥", "æœåŠ¡çŠ¶æ€"],
                model=self.model,
            ),
            CollectorItem(
                title="AIè¡Œä¸šåŠ¨æ€å…³æ³¨å»ºè®®",
                summary="å»ºè®®å…³æ³¨OpenAIã€Claudeã€Googleã€Grokç­‰ä¸»è¦AIå…¬å¸çš„å®˜æ–¹åŠ¨æ€ã€‚",
                content="åœ¨AIæ–°é—»æ”¶é›†æœåŠ¡ä¸å¯ç”¨æœŸé—´ï¼Œå»ºè®®ç›´æ¥å…³æ³¨ä»¥ä¸‹æ¸ é“ï¼š\n1. OpenAIå®˜æ–¹åšå®¢å’ŒTwitter\n2. Anthropic Claudeæ›´æ–°\n3. Google AIç ”ç©¶è¿›å±•\n4. Grok AIå‘å¸ƒåŠ¨æ€\n5. ä¸»è¦ç§‘æŠ€åª’ä½“çš„AIç‰ˆå—",
                url="",
                source=self.get_source_name(),
                author="ç³»ç»Ÿ",
                published_at=datetime.now(),
                tags=["AIåŠ¨æ€", "å…³æ³¨å»ºè®®"],
                model=self.model,
            ),
        ]

        return fallback_items

    def _parse_ai_response(self, content: str) -> List[CollectorItem]:
        """è§£æAIè¿”å›çš„æ–°é—»å†…å®¹"""
        items = []
        
        self.logger.info("ğŸ” è§£æå™¨æ­¥éª¤1: æ£€æŸ¥è¾“å…¥å†…å®¹")
        self.logger.info(f"ğŸ“Š åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        self.logger.info(f"ğŸ“„ åŸå§‹å†…å®¹å‰200å­—ç¬¦: {content[:200]}")

        # æ­¥éª¤2ï¼šæ¸…ç†å†…å®¹
        self.logger.info("ğŸ” è§£æå™¨æ­¥éª¤2: æ¸…ç†AIæ€è€ƒæ ‡ç­¾")
        original_length = len(content)
        content = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL)
        
        self.logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œä» {original_length} å­—ç¬¦ç¼©å‡åˆ° {len(content)} å­—ç¬¦")
        if len(content) < original_length:
            self.logger.info(f"ğŸ—‘ï¸ ç§»é™¤äº† {original_length - len(content)} å­—ç¬¦çš„æ€è€ƒå†…å®¹")
        
        if len(content.strip()) > 0:
            self.logger.info(f"ğŸ“ æ¸…ç†åå†…å®¹å‰300å­—ç¬¦: {content[:300]}")
        else:
            self.logger.error(f"âŒ æ¸…ç†åå†…å®¹ä¸ºç©ºï¼")
        
        # æ­¥éª¤3ï¼šå°è¯•è§£æå¤šæ¡æ–°é—»
        self.logger.info("ğŸ” è§£æå™¨æ­¥éª¤3: å°è¯•è§£æå¤šæ¡æ–°é—»")
        
        if len(content.strip()) > 100:
            self.logger.info(f"âœ… å†…å®¹é•¿åº¦è¶³å¤Ÿ({len(content.strip())} > 100)ï¼Œå¼€å§‹è§£æ")
            self.logger.info(f"ğŸ“ å†…å®¹å‰500å­—ç¬¦: {content[:500]}")
        else:
            self.logger.warning(f"âš ï¸ å†…å®¹é•¿åº¦ä¸è¶³({len(content.strip())} <= 100)ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
        
        # æ­¥éª¤4ï¼šå¤šç§è§£æç­–ç•¥
        self.logger.info("ğŸ” è§£æå™¨æ­¥éª¤4: å°è¯•å¤šç§è§£æç­–ç•¥")
        
        # ç­–ç•¥1: æŒ‰ç¼–å·åˆ†å‰² (1. 2. 3. ...)
        self.logger.info("ğŸ“‹ ç­–ç•¥1: æŒ‰ç¼–å·åˆ†å‰²æ–°é—»")
        news_sections = re.split(r"\n(?=\d+\.)", content)
        if len(news_sections) > 1:
            self.logger.info(f"âœ… ç­–ç•¥1æˆåŠŸï¼šæ‰¾åˆ° {len(news_sections)} ä¸ªç¼–å·æ®µè½")
        
        # ç­–ç•¥2: æŒ‰æ ‡é¢˜æ ¼å¼åˆ†å‰² (**æ ‡é¢˜**: å†…å®¹)
        if len(news_sections) <= 1:
            self.logger.info("ğŸ“‹ ç­–ç•¥2: æŒ‰æ ‡é¢˜æ ¼å¼åˆ†å‰²")
            news_sections = re.split(r"\n(?=\*\*[^*]+\*\*)", content)
            if len(news_sections) > 1:
                self.logger.info(f"âœ… ç­–ç•¥2æˆåŠŸï¼šæ‰¾åˆ° {len(news_sections)} ä¸ªæ ‡é¢˜æ®µè½")
        
        # ç­–ç•¥3: æŒ‰å…¬å¸åç§°åˆ†å‰²
        if len(news_sections) <= 1:
            self.logger.info("ğŸ“‹ ç­–ç•¥3: æŒ‰å…¬å¸åç§°åˆ†å‰²")
            company_pattern = r"\n(?=(?:OpenAI|Claude|Google|Grok|Anthropic|xAI))"
            news_sections = re.split(company_pattern, content)
            if len(news_sections) > 1:
                self.logger.info(f"âœ… ç­–ç•¥3æˆåŠŸï¼šæ‰¾åˆ° {len(news_sections)} ä¸ªå…¬å¸æ®µè½")
        
        # å¯»æ‰¾æ—¥æœŸæ ‡è®°ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
        date_pattern = r"(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)"
        date_match = re.search(date_pattern, content)
        if date_match:
            content = content[date_match.start() :]

        self.logger.info(f"ğŸ” è§£æå™¨æ­¥éª¤5: å¤„ç† {len(news_sections)} ä¸ªæ®µè½")
        
        for i, section in enumerate(news_sections):
            section = section.strip()
            if not section or len(section) < 20:
                self.logger.info(f"â­ï¸ è·³è¿‡æ®µè½{i+1}: é•¿åº¦ä¸è¶³({len(section)})")
                continue

            self.logger.info(f"ğŸ“ å¤„ç†æ®µè½{i+1}: {section[:100]}...")
            
            # æå–æ–°é—»æ ‡é¢˜å’Œå†…å®¹
            lines = section.split("\n")
            title_line = lines[0] if lines else ""

            # å¤šç§æ ‡é¢˜æå–ç­–ç•¥
            title = self._extract_title(title_line, section)
            
            if title and len(title) > 5:
                # æ„å»ºå®Œæ•´å†…å®¹
                full_content = "\n".join(lines[1:]) if len(lines) > 1 else section
                
                # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œä½¿ç”¨æ•´ä¸ªæ®µè½
                if len(full_content.strip()) < 50:
                    full_content = section

                # åˆ›å»ºæ–°é—»é¡¹
                item = CollectorItem(
                    title=title,
                    summary=(
                        full_content[:200] + "..."
                        if len(full_content) > 200
                        else full_content
                    ),
                    content=full_content,
                    url="",
                    source=self.get_source_name(),
                    author=f"{self.model} AIåŠ©æ‰‹",
                    published_at=datetime.now(),
                    tags=["AIæ–°é—»", "ç§‘æŠ€åŠ¨æ€"],
                    model=self.model,
                )
                items.append(item)
                self.logger.info(f"âœ… åˆ›å»ºæ–°é—»é¡¹{len(items)}: {title[:50]}...")
            else:
                self.logger.info(f"â­ï¸ è·³è¿‡æ®µè½{i+1}: æ— æœ‰æ•ˆæ ‡é¢˜")

        # ğŸ”„ å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–æ–°é—»ï¼Œå°è¯•ç®€å•åˆ†å‰²
        if not items and content:
            # æŒ‰åŒæ¢è¡Œåˆ†å‰²
            sections = content.split("\n\n")
            for section in sections:
                section = section.strip()
                if len(section) > 30 and self._contains_ai_keywords(section):
                    items.append(
                        CollectorItem(
                            title=f"AIæ–°é—» - {datetime.now().strftime('%Y-%m-%d')}",
                            summary=(
                                section[:200] + "..." if len(section) > 200 else section
                            ),
                            content=section,
                            url="",
                            source=self.get_source_name(),
                            author="AIåŠ©æ‰‹",
                            published_at=datetime.now(),
                            tags=["AI", "ç§‘æŠ€æ–°é—»"],
                            model=self.model,
                        )
                    )

        # ğŸ”„ æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        if not items and content:
            items.append(
                CollectorItem(
                    title=f"AIæ–°é—»æ‘˜è¦ - {datetime.now().strftime('%Y-%m-%d')}",
                    summary=content[:200] + "..." if len(content) > 200 else content,
                    content=content,
                    url="",
                    source=self.get_source_name(),
                    author="AIåŠ©æ‰‹",
                    published_at=datetime.now(),
                    tags=["AI", "ç§‘æŠ€æ–°é—»", "æ¯æ—¥æ‘˜è¦"],
                    model=self.model,
                )
            )

        return items[:10]  # ğŸ”„ é™åˆ¶è¿”å›æœ€å¤š10æ¡æ–°é—»

    def _is_valid_news_title(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–°é—»æ ‡é¢˜"""
        # è¿‡æ»¤æ‰æ€è€ƒè¿‡ç¨‹
        if any(
            keyword in line.lower()
            for keyword in ["<think>", "first,", "let me", "i should", "i need"]
        ):
            return False

        # åŒ¹é…ç¼–å·å¼€å¤´çš„æ–°é—»
        if re.match(r"^\d+\.\s*", line):
            return True

        # åŒ…å«AIå…¬å¸åç§°ä¸”é•¿åº¦åˆç†
        ai_companies = [
            "OpenAI",
            "Claude",
            "Google",
            "Grok",
            "Microsoft",
            "Meta",
            "Anthropic",
        ]
        if any(company in line for company in ai_companies) and 20 < len(line) < 150:
            return True

        return False

    def _contains_ai_keywords(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«AIå…³é”®è¯"""
        keywords = [
            "AI",
            "äººå·¥æ™ºèƒ½",
            "OpenAI",
            "Claude",
            "Google",
            "Grok",
            "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ ",
        ]
        return any(keyword in text for keyword in keywords)

    def _extract_title(self, title_line: str, full_section: str) -> str:
        """å¤šç§ç­–ç•¥æå–æ–°é—»æ ‡é¢˜"""
        if not title_line:
            return ""
            
        # ç­–ç•¥1: **æ ‡é¢˜** æ ¼å¼
        bold_match = re.search(r'\*\*([^*]+)\*\*', title_line)
        if bold_match:
            title = bold_match.group(1).strip()
            if len(title) > 5:
                return title
        
        # ç­–ç•¥2: ç¼–å·æ ¼å¼ (1. æ ‡é¢˜, 2. æ ‡é¢˜)
        number_match = re.search(r'^\d+[.ã€)\s]+(.+?)(?:\*\*|ï¼š|:|\n|$)', title_line)
        if number_match:
            title = number_match.group(1).strip()
            if len(title) > 5:
                return title
        
        # ç­–ç•¥3: å…¬å¸åå¼€å¤´
        company_match = re.search(r'^(OpenAI|Claude|Google|Grok|Anthropic|xAI)[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]?', title_line)
        if company_match:
            title = company_match.group(0).strip()
            if len(title) > 10:
                return title
        
        # ç­–ç•¥4: ç›´æ¥ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜è¡Œ
        cleaned = self._clean_title(title_line)
        if len(cleaned) > 5 and self._is_valid_news_title(title_line):
            return cleaned
        
        # ç­–ç•¥5: ä»æ®µè½ä¸­æå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ ‡é¢˜
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.]', full_section)
        if sentences and len(sentences[0].strip()) > 10:
            return sentences[0].strip()[:100]
        
        return ""

    def _clean_title(self, title: str) -> str:
        """æ¸…ç†æ ‡é¢˜æ ¼å¼"""
        # ç§»é™¤ç¼–å·å’Œç‰¹æ®Šå­—ç¬¦
        title = re.sub(r"^\d+[.ã€)\s]+", "", title)
        title = re.sub(r"^[\*\-â€¢\s]+", "", title)
        title = title.strip()

        # é™åˆ¶æ ‡é¢˜é•¿åº¦
        if len(title) > 100:
            title = title[:100] + "..."

        return title

    async def run(self) -> Dict[str, Any]:
        """è¿è¡Œæ”¶é›†å™¨"""
        if not self.enabled:
            return {
                "success": False,
                "error": "æ”¶é›†å™¨å·²ç¦ç”¨",
                "count": 0,
                "execution_time": 0,
            }

        start_time = time.time()

        try:
            self.logger.info(f"å¼€å§‹è¿è¡ŒAIæ–°é—»æ”¶é›†å™¨: {self.name}")

            # æ”¶é›†æ–°é—»é¡¹ç›®
            items = await self.collect()

            execution_time = time.time() - start_time

            if items:
                self.logger.info(f"æˆåŠŸæ”¶é›†åˆ° {len(items)} æ¡AIæ–°é—»")
                return {
                    "success": True,
                    "count": len(items),
                    "items": items,
                    "execution_time": execution_time,
                    "source": self.get_source_name(),
                }
            else:
                return {
                    "success": False,
                    "error": "æœªæ”¶é›†åˆ°ä»»ä½•æ–°é—»",
                    "count": 0,
                    "execution_time": execution_time,
                }

        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"AIæ–°é—»æ”¶é›†å™¨è¿è¡Œå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            # æ‰“å°è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": error_msg,
                "count": 0,
                "execution_time": execution_time,
            }

    def __str__(self):
        return f"AINewsCollector(name={self.name}, model={self.model})"
